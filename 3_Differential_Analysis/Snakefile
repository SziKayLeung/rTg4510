import os
import sys

shell.executable("/bin/bash")

configfile: "config.yaml"

# https://stackoverflow.com/questions/40398091/how-to-do-a-partial-expand-in-snakemake
rule all:
    input:
        expand(config["isoseq_output_dir"] + "/CLUSTER/{sample}_clustered.fastq", sample=config["samples"].keys()),
        expand(config["post_isoseq_output_dir"] + "/MAPPING/{sample}.clustered.hq.fastq.sorted.sam", sample=config["samples"].keys()),
        expand(config["isoseq_output_dir"] + "/TOFU/{sample}.collapsed.filtered.rep.fa", sample=config["samples"].keys()),
        expand(config["post_isoseq_output_dir"] + "/TOFU/{sample}.collapsed.filtered.rep.fa", sample=config["samples"].keys()),
        config["post_isoseq_output_dir"] + "/CHAIN/all_samples.chained.rep.fq",
        config["post_isoseq_output_dir"] + "/KALLISTO/" + config["Kallisto_Output_Name"] + "_R1.fq",
        config["post_isoseq_output_dir"] + "/KALLISTO/" + "abundance.tsv",
        config["post_isoseq_output_dir"] + "/SQANTI/Sqanti.filter.log"


##### IsoSeq3
rule ccs:
    input:
       subreads_bam = lambda wc: config["raw_dir"] + "/" + config["samples"][wc.sample]
    output:
       ccs_bam = config["isoseq_output_dir"] + "/CCS/{sample}_ccs.bam",
       ccs_report_file = config["isoseq_output_dir"] + "/CCS/{sample}_ccs_report.txt"
    conda:
       "isoseq3_snakemake.yaml"
    params:
       cpasses = config["ccs_passes"]
    shell:"""
       echo "Processing CCS on {input.subreads_bam}, called to {wildcards.sample}";
       ccs --num-threads=32 --noPolish --minPasses={params.cpasses} {input.subreads_bam} {output.ccs_bam} --reportFile {output.ccs_report_file}
    """

rule lima:
    input:
       ccs_bam = config["isoseq_output_dir"] + "/CCS/{sample}_ccs.bam",
       primer_fasta = config["primer_fasta"]
    output:
       fl_bam = config["isoseq_output_dir"] + "/LIMA/{sample}_fl.primer_5p--primer_3p.bam"
    params:
       output_dir = directory(config["isoseq_output_dir"] + "/LIMA/")
    conda:
       "isoseq3_snakemake.yaml"
    shell:"""
       cd {params.output_dir};
       lima {input.ccs_bam} {input.primer_fasta} {wildcards.sample}_fl.bam --isoseq --dump-clips --dump-removed --peek-guess
    """

rule refine:
    input:
       fl_bam = config["isoseq_output_dir"] + "/LIMA/{sample}_fl.primer_5p--primer_3p.bam",
       primer_fasta = config["primer_fasta"]
    output:
       flnc_bam = config["isoseq_output_dir"] + "/REFINE/{sample}_flnc.bam",
       log_file = config["isoseq_output_dir"] + "/REFINE/{sample}_refine.log"
    conda:
       "isoseq3_snakemake.yaml"
    shell:"""
       isoseq3 refine {input.fl_bam} {input.primer_fasta} {output.flnc_bam} --require-polya 2> {output.log_file}
    """

rule cluster:
    input:
       flnc_bam = config["isoseq_output_dir"] + "/REFINE/{sample}_flnc.bam"
    output:
       clustered_report = config["isoseq_output_dir"] + "/CLUSTER/{sample}_clustered.cluster_report.csv",
       clustered_fastq = config["isoseq_output_dir"] + "/CLUSTER/{sample}_clustered.fasta"
    params:
       output_dir = directory(config["isoseq_output_dir"] + "/CLUSTER/")
    conda:
       "isoseq3_snakemake.yaml"
    log:
       config["isoseq_output_dir"] + "/CLUSTER/{sample}_cluster.log"
    shell:"""
       cd {params.output_dir};
       isoseq3 cluster {input.flnc_bam} {wildcards.sample}_clustered.bam --verbose;
       gunzip {wildcards.sample}_clustered.fasta.gz;
    """

rule fasta2fastq:
    input:
       clustered_fasta = config["isoseq_output_dir"] + "/CLUSTER/{sample}_clustered.fasta"
    output:
       clustered_fastq = config["isoseq_output_dir"] + "/CLUSTER/{sample}_clustered.fastq"
    params:
       cupcake_sequence = "/gpfs/mrc0/projects/Research_Project-MRC148213/sl693/softwares/Post_Isoseq3/cDNA_Cupcake/sequence"
    conda:
       "sqanti2_py3_snakemake.yaml"
    shell:"""
       python {params.cupcake_sequence}/fa2fq.py {input.clustered_fasta} > {output.clustered_fastq}
    """

##################################### Post-IsoSeq3
##### Mapping with Minimap2
rule map_reads:
    input:
       reference = config["transcriptome"],
       fastq = config["isoseq_output_dir"] + "/CLUSTER/{sample}_clustered.fastq"
    output:
       mapped_sam = config["post_isoseq_output_dir"] + "/MAPPING/{sample}.clustered.hq.fastq.sam",
       log_file = config["post_isoseq_output_dir"] + "/MAPPING/{sample}.clustered.hq.fastq.sam.log"
    conda:
       "sqanti2_py3_snakemake.yaml"
    shell:"""
       minimap2 -t 32 -ax splice -uf --secondary=no -C5 -O6,24 -B4 {input.reference} {input.fastq} > {output.mapped_sam} 2> {output.log_file}
    """

rule sort_map_reads:
    input:
        mapped_sam = config["post_isoseq_output_dir"] + "/MAPPING/{sample}.clustered.hq.fastq.sam"
    output:
        sorted_mapped_sam = config["post_isoseq_output_dir"] + "/MAPPING/{sample}.clustered.hq.fastq.sorted.sam"
    conda:
       "sqanti2_py3_snakemake.yaml"
    shell:"""
       sort -k 3,3 -k 4,4n {input.mapped_sam} > {output.sorted_mapped_sam}
    """

##### TOFU Collapse
rule tofu_collapse:
    input:
        fq = config["isoseq_output_dir"] + "/CLUSTER/{sample}_clustered.fastq",
        sorted_mapped_sam = config["post_isoseq_output_dir"] + "/MAPPING/{sample}.clustered.hq.fastq.sorted.sam",
        clustered_report = config["isoseq_output_dir"] + "/CLUSTER/{sample}_clustered.cluster_report.csv"
    output:
        collapse_log_file = config["post_isoseq_output_dir"] + "/TOFU/{sample}_collapse.log",
        abundance_log_file = config["post_isoseq_output_dir"] + "/TOFU/{sample}_abundance.log",
        filter_log_file = config["post_isoseq_output_dir"] + "/TOFU/{sample}_filter.log",
        collapsed_fa = config["post_isoseq_output_dir"] + "/TOFU/{sample}.collapsed.filtered.rep.fa",
        group = config["post_isoseq_output_dir"] + "/TOFU/{sample}.collapsed.group.txt",
        abundance = config["post_isoseq_output_dir"] + "/TOFU/{sample}.collapsed.filtered.abundance.txt",
        fasta = config["post_isoseq_output_dir"] + "/TOFU/{sample}.collapsed.filtered.rep.fq",
        gff = config["post_isoseq_output_dir"] + "/TOFU/{sample}.collapsed.filtered.gff",
        chain_group = config["post_isoseq_output_dir"] + "/CHAIN/{sample}/Sample.collapsed.group.txt",
        chain_abundance = config["post_isoseq_output_dir"] + "/CHAIN/{sample}/Sample.collapsed.filtered.abundance.txt",
        chain_fasta = config["post_isoseq_output_dir"] + "/CHAIN/{sample}/Sample.collapsed.filtered.rep.fq",
        chain_gff = config["post_isoseq_output_dir"] + "/CHAIN/{sample}/Sample.collapsed.filtered.gff"
    params:
        output_dir = config["post_isoseq_output_dir"] + "/TOFU/",
        dir = directory(config["post_isoseq_output_dir"] + "/CHAIN/{sample}")
    conda:
       "sqanti2_py3_snakemake.yaml"
    shell:"""
       cd {params.output_dir};
       collapse_isoforms_by_sam.py --fq --input {input.fq} -s {input.sorted_mapped_sam} --dun-merge-5-shorter -o {wildcards.sample} &> {output.collapse_log_file};
       get_abundance_post_collapse.py {wildcards.sample}.collapsed {input.clustered_report} 2> {output.abundance_log_file};
       filter_away_subset.py {wildcards.sample}.collapsed 2> {output.filter_log_file};
       seqtk seq -a {wildcards.sample}.collapsed.filtered.rep.fq > {output.collapsed_fa};
       cp {output.group} {output.chain_group};
       cp {output.abundance} {output.chain_abundance};
       cp {output.fasta} {output.chain_fasta};
       cp {output.gff} {output.chain_gff}
    """

rule chain_collapse:
    input:
        config_file = config["chain_config"]
    output:
        log_file =  config["isoseq_output_dir"] + "/CHAIN/Chained_Configuration.log",
        chained_fa = config["post_isoseq_output_dir"] + "/CHAIN/all_samples.chained.rep.fq"
    conda:
       "sqanti2_py3_snakemake.yaml"
    params:
        cupcake_counting = "/gpfs/mrc0/projects/Research_Project-MRC148213/sl693/softwares/Post_Isoseq3/cDNA_Cupcake/cupcake/tofu/counting",
        output_dir = config["post_isoseq_output_dir"] + "/CHAIN/"
    shell: """
        cd {params.output_dir};
        python {params.cupcake_counting}/chain_samples.py {input.config_file} count_fl --dun-merge-5-shorter 2> {output.log_file}
    """


##### RNASeq Expression Input
rule RNASeq_merge_fastq:
    params:
        script = "/gpfs/mrc0/projects/Research_Project-MRC148213/sl693/Scripts/IsoSeq_Tg4510/1_Transcriptome_Annotation/Snakemake/rnaseq_merge.sh",
        rnaseq_dir = config["RNASeq_Filtered_dir"],
        kallisto_output_dir = config["post_isoseq_output_dir"] + "/KALLISTO/",
        name = config["Kallisto_Output_Name"],
        samples = config["RNASeq_Expression_Samples"]
    output:
        R1_merged = config["post_isoseq_output_dir"] + "/KALLISTO/" + config["Kallisto_Output_Name"] + "_R1.fq",
        R2_merged = config["post_isoseq_output_dir"] + "/KALLISTO/" + config["Kallisto_Output_Name"] + "_R2.fq"
    shell:"""
        source {params.script};
        # RNASeq_merge_fastq <RNASEQ_input_dir> <Kallisto_output_dir> <sample_prefix_output_name> <samples...>
        RNASeq_merge_fastq {params.rnaseq_dir} {params.kallisto_output_dir} {params.name} {params.samples}
    """


rule kallisto:
    input:
        R1_merged = config["post_isoseq_output_dir"] + "/KALLISTO/" + config["Kallisto_Output_Name"] + "_R1.fq",
        R2_merged = config["post_isoseq_output_dir"] + "/KALLISTO/" + config["Kallisto_Output_Name"] + "_R2.fq",
        chained_fa = config["post_isoseq_output_dir"] + "/CHAIN/all_samples.chained.rep.fq"
    output:
        kallisto_index = config["post_isoseq_output_dir"] + "/KALLISTO/" + config["Kallisto_Output_Name"] + "_Kallisto.idx",
        index_log_file = config["post_isoseq_output_dir"] + "/KALLISTO/" + config["Kallisto_Output_Name"] + "_Kallisto.index.log",
        quant_log_file = config["post_isoseq_output_dir"] + "/KALLISTO/" + config["Kallisto_Output_Name"] + "_Kallisto.quant.log",
        tsv = config["post_isoseq_output_dir"] + "/KALLISTO/" + config["Kallisto_Output_Name"] + "_abundance.tsv"
    params:
        kallisto_dir = directory(config["post_isoseq_output_dir"] + "/KALLISTO"),
        output_name = config["Kallisto_Output_Name"]
    conda:
        "sqanti2_py3_snakemake.yaml"
    shell:"""
        kallisto index -i {params.kallisto_dir}/{params.output_name}_Kallisto.idx {input.chained_fa} 2> {output.index_log_file}
        kallisto quant -i {params.kallisto_dir}/{params.output_name}_Kallisto.idx --fr-stranded {input.R1_merged} --rf-stranded {input.R2_merged} -o {params.kallisto_dir} 2> {output.quant_log_file};

        # problem: retained co-ordinates, which does not input well into SQANTI2
        # solution: retain the PB.ID
        while read line ; do
            first=$( echo "$line" |cut -d\| -f1 ) # each read line, and remove the first part i.e. PacBio ID
            rest=$( echo "$line" | cut -d$'\t' -f2-5 ) #save the remaining columns
            echo $first $rest # concatenate
        done < {params.kallisto_dir}/abundance.tsv > {params.kallisto_dir}/temp_abundance.tsv

        header=$(head -n 1 {params.kallisto_dir}/abundance.tsv )
        sed -i '1d' {params.kallisto_dir}/temp_abundance.tsv # remove header of temp.file to be replaced
        echo $header > foo
        cat foo {params.kallisto_dir}/temp_abundance.tsv > {params.kallisto_dir}/{params.output_name}_abundance.tsv

        rm {params.kallisto_dir}/temp_abundance.tsv
        rm foo
    """

rule sqanti_qc:
    input:
        chained_fa = config["post_isoseq_output_dir"] + "/CHAIN/all_samples.chained.gff",
        chained_abundance = config["post_isoseq_output_dir"] + "/CHAIN/all_samples.chained_count.txt",
        reference_fa = config["transcriptome"],
        reference_gtf = config["annotation"],
        kallisto_tsv = config["post_isoseq_output_dir"] + "/KALLISTO/" + "abundance.tsv"
    output:
        log_file = config["post_isoseq_output_dir"] + "/SQANTI/Sqanti.qc.log",
        sqanti_classification = config["post_isoseq_output_dir"] + "/SQANTI/all_samples.chained_classification.txt",
        sqanti_fasta = config["post_isoseq_output_dir"] + "/SQANTI/all_samples.chained_corrected.fasta",
        sqanti_gtf = config["post_isoseq_output_dir"] + "/SQANTI/all_samples.chained_corrected.gtf",
    params:
        cupcake_sequence = "/gpfs/mrc0/projects/Research_Project-MRC148213/sl693/softwares/Post_Isoseq3/cDNA_Cupcake/sequence",
        sqanti_dir = config["isoseq_output_dir"] + "/SQANTI/",
        sqanti_software_dir = "/gpfs/mrc0/projects/Research_Project-MRC148213/sl693/softwares/SQANTI3",
        cage_peak = config["cage_peaks"],
        polyA = config["polyA_motif"],
        junction_coverage = config["RNASeq_STAR_files"]
    conda:
       "sqanti2_py3_snakemake.yaml"
    shell:"""
        cd {params.sqanti_dir};
        export PYTHONPATH=$PYTHONPATH:{params.cupcake_sequence};
        python {params.sqanti_software_dir}/sqanti3_qc.py -t 30 --gtf {input.chained_fa} {input.reference_gtf} {input.reference_fa} -fl {input.chained_abundance} --cage_peak {params.cage_peak} --polyA_motif_list {params.polyA} -c {params.junction_coverage} -e {input.kallisto_tsv} 2> {output.log_file}
    """

rule sqanti_filter:
    input:
        sqanti_classification = config["post_isoseq_output_dir"] + "/SQANTI/all_samples.chained_classification.txt",
        sqanti_fasta = config["post_isoseq_output_dir"] + "/SQANTI/all_samples.chained_corrected.fasta",
        sqanti_gtf = config["post_isoseq_output_dir"] + "/SQANTI/all_samples.chained_corrected.gtf",
    output:
        log_file = config["post_isoseq_output_dir"] + "/SQANTI/Sqanti.filter.log"
    params:
        cupcake_sequence = "/gpfs/mrc0/projects/Research_Project-MRC148213/sl693/softwares/Post_Isoseq3/cDNA_Cupcake/sequence",
        sqanti_dir = config["isoseq_output_dir"] + "/SQANTI/",
        sqanti_software_dir = "/gpfs/mrc0/projects/Research_Project-MRC148213/sl693/softwares/SQANTI3",
    conda:
       "sqanti2_py3_snakemake.yaml"
    shell:"""
        cd {params.sqanti_dir};
        export PYTHONPATH=$PYTHONPATH:{params.cupcake_sequence};
        python {params.sqanti_software_dir}/sqanti3_RulesFilter.py -a 0.6 -c 3 {input.sqanti_classification} {input.sqanti_fasta} {input.sqanti_gtf} 2> {output.log_file}
    """
